from basicsr.archs.mambairv2_arch import MambaIRv2
import torch
import torch.nn as nn
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MambaIRv2(
    img_size=64,
    patch_size=1,
    in_chans=1,        # ä¼šè¢«å¼ºåˆ¶ä¸º 1
    embed_dim=174,
    d_state=16,
    depths=(6, 6, 6, 6, 6, 6),
    num_heads= [6, 6, 6, 6, 6, 6],
    window_size=16,
    inner_rank=64,
    num_tokens=128,
    convffn_kernel_size=5,
    mlp_ratio=2.0,
    upsampler='',      # ä¼šè¢«å¼ºåˆ¶ä¸º ''
    upscale=1,         # ä¼šè¢«å¼ºåˆ¶ä¸º 1
    resi_connection='1conv'

).to(device)

# Load the full checkpoint (could be .pth or .pt file)
checkpoint = torch.load('/content/mambairv2_ColorDN_15.pth', map_location='cpu')

state_dict = checkpoint.get('params', checkpoint)  # 'params' if it's a dict, else the plain dict

# Remove the incompatible keys
to_ignore = []
for k in state_dict.keys():
    # Ignore the first and last conv layers
    if k.startswith('conv_first') or k.startswith('conv_last'):
        to_ignore.append(k)
    # If using conv_after_body or upsampling, you may want to check those too

for k in to_ignore:
    print(f"Skip loading {k}")
    state_dict.pop(k)


# Now load the rest (strict=False allows missing/unmatched keys)
model.load_state_dict(state_dict, strict=False)
# kaiming/he initialization for the first conv layer
nn.init.kaiming_normal_(model.conv_first.weight, mode='fan_out', nonlinearity='relu')
if model.conv_first.bias is not None:
    nn.init.zeros_(model.conv_first.bias)
#zero initialization for the last conv layer
nn.init.zeros_(model.conv_last.weight)
if model.conv_last.bias is not None:
    nn.init.zeros_(model.conv_last.bias)
    
input_tensor = torch.randn(2, 1, 64, 64).to(device)  # è¾“å…¥ä¹Ÿæ”¾åˆ° device
# output = model(input_tensor)
# print(output.shape)  # Should print torch.Size([2, 2, 64, 64])
import os
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as T

class LabColorizationDataset(Dataset):
    def __init__(self, img_dir, img_size=64, max_samples=None):
        self.img_paths = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith(('png','jpg','jpeg'))]
        if max_samples is not None:
            self.img_paths = self.img_paths[:max_samples]  # åªä¿ç•™å‰ max_samples å¼ å›¾ç‰‡
        self.transform = T.Compose([
            T.Resize((img_size,img_size)),
            T.ToTensor()
        ])

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img = Image.open(self.img_paths[idx]).convert('RGB')
        img = self.transform(img)  # [3,H,W], [0,1]
        img_np = np.array(img.permute(1,2,0)) * 255
        img_lab = rgb_to_lab(img_np)
        L = img_lab[:,:,0:1] / 100.0
        ab = (img_lab[:,:,1:] + 128) / 255.0
        import torch
        L = torch.from_numpy(L).permute(2,0,1).float()
        ab = torch.from_numpy(ab).permute(2,0,1).float()
        return L, ab

def rgb_to_lab(img):
    import cv2
    return cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2LAB).astype(np.float32)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

criterion = nn.L1Loss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

img_dir = '/content/drive/MyDrive/PatternAnalysis-2025/data/train2017'
dataset = LabColorizationDataset(img_dir, img_size=64,max_samples=2000)
loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)

num_epochs = 2
for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for step, (L, ab) in enumerate(loader, start=1):
        L, ab = L.to(device), ab.to(device)
        optimizer.zero_grad()
        ab_pred = model(L)
        loss = criterion(ab_pred, ab)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * L.size(0)
        print(f"Epoch {epoch+1} Step {step}/{len(loader)} | loss={loss.item():.6f}")
    print(f"[Epoch {epoch+1}] mean_loss={epoch_loss/len(loader.dataset):.6f}")


torch.save(model.state_dict(), "color_model_lab_trained.pth")
# ===== Inference & Visualization =====
import random
import matplotlib.pyplot as plt
import cv2
import numpy as np
import torch

# å°å·¥å…·ï¼šæŠŠ (L, ab) è¿˜åŸæˆ RGBï¼ˆnumpy uint8, HxWx3ï¼‰
def lab_to_rgb_np(L_t, ab_t):
    """
    L_t: torch.Tensor [1,H,W], å–å€¼èŒƒå›´ [0,1]ï¼Œéœ€è¦å…ˆä¹˜ä»¥ 100
    ab_t: torch.Tensor [2,H,W], å–å€¼èŒƒå›´ [0,1]ï¼Œéœ€è¦å…ˆ*255-128
    """
    L = (L_t.squeeze(0).cpu().numpy() * 100.0).astype(np.float32)        # [H,W]
    ab = (ab_t.cpu().numpy() * 255.0 - 128.0).astype(np.float32)         # [2,H,W]
    a = ab[0, ...]
    b = ab[1, ...]
    lab = np.stack([L, a, b], axis=-1)                                   # [H,W,3] in LAB
    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)                           # float32, 0-255 ish
    rgb = np.clip(rgb, 0, 255).astype(np.uint8)
    return rgb

# 1) è½½å…¥è®­ç»ƒå¥½çš„æƒé‡ï¼ˆä¹Ÿå¯ä»¥çœç•¥è¿™æ­¥ï¼Œå› ä¸ºä½ ä¸Šé¢åˆšè®­ç»ƒå®Œï¼‰
ckpt_path = "color_model_lab_trained.pth"
state = torch.load(ckpt_path, map_location="cpu")
_ = model.load_state_dict(state, strict=False)
model.eval()

# 2) éšæœºæŠ½å– 3 ä¸ªæ ·æœ¬ç´¢å¼•
indices = random.sample(range(len(dataset)), 3)

# 3) æ¨ç†ä¸å¯è§†åŒ–
fig = plt.figure(figsize=(9, 9))  # 3 è¡Œ Ã— 3 åˆ—
plt.subplots_adjust(hspace=0.15, wspace=0.05)

for i, idx in enumerate(indices):
    # ä» dataset å–å‡ºå•æ ·æœ¬ï¼ˆL, abï¼‰ï¼Œä»¥åŠåŸå›¾ RGBï¼ˆä¾¿äºå¯¹æ¯”ï¼‰
    L, ab_gt = dataset[idx]                       # L:[1,64,64], ab:[2,64,64], å½’ä¸€åŒ–è¿‡
    # ä¸ºäº†å±•ç¤ºâ€œåŸå›¾ RGBâ€ï¼Œä»ç£ç›˜è¯»åŸå›¾å† resize åˆ° 64x64
    img_path = dataset.img_paths[idx]
    orig_rgb = cv2.cvtColor(
        cv2.resize(cv2.imread(img_path), (64, 64)),
        cv2.COLOR_BGR2RGB
    )

    with torch.no_grad():
        L_in = L.unsqueeze(0).to(device)          # [1,1,64,64]
        ab_pred = model(L_in)                     # [1,2,64,64]ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å½’ä¸€åŒ–èŒƒå›´
        ab_pred = ab_pred.squeeze(0).clamp(0, 1)  # [2,64,64]ï¼Œç¨³å¦¥èµ·è§ clamp

    # è¿˜åŸé¢„æµ‹çš„ RGB
    pred_rgb = lab_to_rgb_np(L, ab_pred)

    # ç°åº¦å›¾åªæ˜¾ç¤º Lï¼ˆ0-1 ä¹‹é—´ï¼Œcmap='gray'ï¼‰
    gray_img = (L.squeeze(0).cpu().numpy())      # [64,64], 0~1

    # ç”»ä¸‰åˆ—ï¼šç°åº¦ / é¢„æµ‹RGB / åŸå›¾RGB
    # ç°åº¦
    ax = plt.subplot(3, 3, 3*i + 1)
    ax.imshow(gray_img, cmap='gray', vmin=0.0, vmax=1.0)
    ax.set_title(f"Sample {idx} - L")
    ax.axis('off')

    # é¢„æµ‹
    ax = plt.subplot(3, 3, 3*i + 2)
    ax.imshow(pred_rgb)
    ax.set_title("Predicted RGB")
    ax.axis('off')

    # åŸå›¾
    ax = plt.subplot(3, 3, 3*i + 3)
    ax.imshow(orig_rgb)
    ax.set_title("Original RGB")
    ax.axis('off')

plt.show()
import os
import matplotlib.pyplot as plt
import cv2
import numpy as np

# ==== ä¿å­˜æ¨¡å‹æƒé‡ ====
save_path = "/content/drive/MyDrive/color_model_lab_trained_final.pth"
torch.save(model.state_dict(), save_path)
print(f"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {save_path}")

# ==== æ¨ç†å¹¶ä¿å­˜ 3 å¼ å›¾ç‰‡ ====
os.makedirs("/content/drive/MyDrive", exist_ok=True)

model.eval()
indices = [0, 1, 2]  # ä»è®­ç»ƒé›†å–å‰ä¸‰å¼ 
for i, idx in enumerate(indices):
    L, ab_gt = dataset[idx]
    img_path = dataset.img_paths[idx]
    with torch.no_grad():
        L_in = L.unsqueeze(0).to(device)
        ab_pred = model(L_in).squeeze(0).clamp(0,1)

    # è½¬ä¸ºRGB
    def lab_to_rgb_np(L_t, ab_t):
        import cv2
        L = (L_t.squeeze(0).cpu().numpy() * 100.0).astype(np.float32)
        ab = (ab_t.cpu().numpy() * 255.0 - 128.0).astype(np.float32)
        lab = np.stack([L, ab[0], ab[1]], axis=-1)
        rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        return np.clip(rgb, 0, 255).astype(np.uint8)

    pred_rgb = lab_to_rgb_np(L, ab_pred)
    gray_img = (L.squeeze(0).cpu().numpy() * 255).astype(np.uint8)
    orig_rgb = cv2.cvtColor(
        cv2.resize(cv2.imread(img_path), (64, 64)),
        cv2.COLOR_BGR2RGB
    )

    # ä¿å­˜ä¸‰å¼ å›¾ç‰‡
    cv2.imwrite(f"/content/drive/MyDrive/infer_results/sample{i+1}_gray.png", gray_img)
    cv2.imwrite(f"/content/drive/MyDrive/infer_results/sample{i+1}_pred.png", cv2.cvtColor(pred_rgb, cv2.COLOR_RGB2BGR))
    cv2.imwrite(f"/content/drive/MyDrive/infer_results/sample{i+1}_orig.png", cv2.cvtColor(orig_rgb, cv2.COLOR_RGB2BGR))
    print(f"âœ… å·²ä¿å­˜ sample{i+1} ä¸‰å¼ å›¾ç‰‡åˆ° /content/infer_results")

print("ğŸ‰ æ‰€æœ‰ç»“æœå·²ä¿å­˜å®Œæˆï¼")
